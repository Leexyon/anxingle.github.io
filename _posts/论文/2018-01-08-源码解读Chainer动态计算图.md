---
layout: post
title: 源码解读动态计算图
category: 论文
tags: 计算机科学
keywords: 
description: 
---


# 经典深度学习框架      
经典的深度学习框架比如caffe，mxnet，tensorflow等都是使用的静态计算图模型（当然最近tensorflow推出新的Execution模式—Eager，mxnet也开始使用新的前端框架Gluon…这是后话），也就是先定义一个计算图，然后往里面“喂”数据，这是经典的Define-and-Run模式…..（我也不知道继续该怎么编了，反正大家都知道怎么用tensorflow和caffe，就是和我们用numpy不一样）直到[Chainer](http://learningsys.org/papers/LearningSys_2015_paper_33.pdf)横空出世，引入了动态图机制，让深度学习研究工作者眼前焕然一新，pytorch也来了，春天近了。   



可以说Chainer的引入的动态图机制是革命性的，不得不佩服日本人开发的这款框架，简介好用，代码基于python，易于阅读，实在是居家旅行不可不少的休闲阅读佳作。不多说了，我们拿出[论文](http://learningsys.org/papers/LearningSys_2015_paper_33.pdf)，结合[代码](https://github.com/chainer/chainer)，一睹为快。



<center><embed src="/public/img/pdf/chainer.pdf" width="850" height="600"></center>



## 静态图的问题

现在的深度学习研究进展非常之快，各种网络不断推陈出新，比如：

> 首先我们来看一下目前流行框架的开发团队和他们开发框架的驱动力：

> Caffe：贾扬清和伯克利视觉实验室的小伙伴们开发。开始主要是自己用，属于需求驱动。

> Torch：Yann LeCun的学生。需求驱动。

> Theano：Yoshua Benjio的学生。用于自己科研，但是也发了系统的paper，属于需求＋科研驱动。

> Tensorflow：Jeff Dean带领的Google员工，主要是系统出身。源于Google在AI领域的布局需求，资本驱动。

> Neon：nervana员工，作为创业公司的产品。资本驱动。

> MXNet：DMLC（主要是华人机器学习和分布式系统学生）的小伙伴。主要是Minerva，Purine，和cxxnet的开发团队合在一起，一半搞机器学习的，一半搞系统的。需求＋兴趣驱动。

> 剩下还有很多搞系统的人出于兴趣或者科研目的开发的框架，但大多没有流行起来，就不再赘述了。

> 可以看出，除了Google强推的Tensorflow，大多都是从自用和兴趣开始的。而* Tensorflow的开发经费比其他所有框架的经费加起来还要多出几十倍，但是一年下来并没能一统江湖 *。可见需求驱动的力量，所谓“需要是发明之母”。
>
> 引自：[做底层 AI 框架和做上层 AI 应用，哪个对自己的学术水平（或综合能力）促进更大？]( https://www.zhihu.com/question/52820499/answer/138716362)

现在的深度学习框架，已经远远跟不上发展的需求。这使得定义新的网络结构非常的困难：因为现有的框架设计之初是为已经有多那些网络结构来设计的。而且要命的是：



## 迭代更新

我们假定此时（S1状态）的决策是基于此时的行为带来的奖励，加上这个行为后的“结果”。也就是说如果睡觉（a1），那么奖励比较大（做个美梦），但是下一个状态的“后果”却不好；如果学习（a2），那么奖励很小（学习很累啊），但是下一个状态的“后果”却很好。所以我们得到S1状态下采取a1（a2则替换为对应的R）的Q值为：$$Q_{target}(S_{1},a_{1})=R_{1} + 𝛾*maxQ(S_{2})$$   
但是实际上的Q值$Q_{eval}(S_{1},a_{1})$刚开始的时候却是我们随便初始化的。所以这里我们就有了一个优化目标，就是让真实的$Q{S_{1},a_{1}}$等于$R_{1} + 𝛾*maxQ(S_{2})$。而
Q目标和Q现实之间有一个差距（理想与现实之间总是有差距的），我们定义为$$差距=Q_{target}(S_{1},a_{1})-Q_{eval}(S_{1},a_{1})$$。我们让$$Q(S_{1},a_{1}) := Q(S_{1},a_{1}) + 𝛼*差距$$。    

+ 我们有了$Q_{target}(S_{1},a_{1})=R_{1} + 𝛾*maxQ(S)$,直接按照这个来更新Q值不好吗？          
+ 为什么更新的时候$Q_{target}(S_{1},a_{1})=R_{1} + 𝛾*maxQ(S)$中取的是max值呢？      
+ 这里的为什么加上一个𝛼呢？它有什么具体含义呢？     

我们需要明白，在开始的时候，我们随便初始化了一个Q表，里面都是随机值，并不精确，我们只能通过每一次的行动，让Q值不断的逼近$R + 𝛾*maxQ(S)$。，上述更新迭代公式，不正像极了反向传导中的梯度更新吗？
我们引入Q-learning的算法流程：         

<img src="https://raw.githubusercontent.com/anxingle/anxingle.github.io/master/public/img/ML/RL_4.png" width="400">

## 实例

（未完待续）            
<img src="https://raw.githubusercontent.com/anxingle/anxingle.github.io/5d487280881d954ddc87cdb1cb44395d3b3e6291/public/img/money_1.png" width="190"><img src="https://raw.githubusercontent.com/anxingle/anxingle.github.io/master/public/img/money_2.jpeg" width="230">






