---
layout: post
title: 简单的增强学习实例讲解：概念篇
category: 论文
tags: 计算机科学
keywords: 
description: 
---


# 什么是增强学习      
简单通俗来讲就是Agent在Environment中探索找到最优策略Policy，使得Reward最多的学习策略（游戏）。它是一个MDP（Markov Decision Process）马尔可夫决策过程：一个状态$S_{t}$是MarKov当且仅当
$P(S_{t+1}|S_{t})=P(S_{t+1}|S_{t},S_{t-1},...,S_{0})$。再通俗点讲就是未来只取决于现在。         

不过扯这么个概念无非显摆数学很重要，[西瓜树](https://book.douban.com/subject/26708119/)讲的更加的深入，我就是从[西瓜树](https://book.douban.com/subject/26708119/)学的。不过按我这里讲的，你可以不用再从头推这么多大头的公式就可以大致理解算法过程。      
## Q表
我们假设刚开始遇见一个新的环境，不知道这里的生存法则，那就需要探索来找到每种“动作”所产生的“效益”（反馈），以后在这个环境中的行为就可以基于长时间在这里学习到的经验来决策。假设我们的生活就像游戏：和下面的大雄一样，刚来到这个世界上，父母还真就没告诉我们那些大道理“好好学习才能挣大钱”，那么我们就需要自己来长时间的反复探索(一切可以重来，但必须从头来过)。     

<img src="https://raw.githubusercontent.com/anxingle/anxingle.github.io/master/public/img/ML/RL_1.png" width="600">      
大雄在$S_{1}$状态有两个选择：$a_{1}$睡觉和$a_{2}$认真学习。他如果选择了睡觉，有可能会梦见得到了静香，而如果好好学习，可能因为自己笨，作业不会做而备受打击。假设他还是选择了睡觉，那么他得到了一个“奖赏”（梦里见静香），达到了另外一个状态$S_{2}$,可是这个状态很不好：大雄因为成绩太差被他妈打了！之后他还可以继续选择“睡觉”或是“学习”.... 这样的循环往复很多次，最终以他娶了胖妹活着静香作为状态的结束。                     

那么，我们定义每一种状态都有一个得分：Q（S，a）。我们可以把上述过程用一个Q表来表示出来，
| | 睡觉 | 学习 |
| ------| ------ | ------ |
| S1 | -2 | -1 |

<img src="https://raw.githubusercontent.com/anxingle/anxingle.github.io/master/public/img/ML/RL_2.png" width="400">       
根据上图，我们知道大雄采取某一种行动还会有额外的“奖励”。虽然睡觉比学习的状态得分低，但是“奖励”高：睡觉可以梦到静香，学习可能会倍受打击。大雄继续采取某个行动后，Q表会再次的“更新”（添加新状态，或者更改值）。大雄脑子一抽风，决定学习了，这次发现学习后不再是啥都不会了，而是成绩有所提高，于是这个状态下的行为终于有了好的效果。于是我们更新这个Q表：            

| | 睡觉 | 学习 |
| ------| ------ | ------ |
| S1 | -2 | -1 |
| S2 | -4 | 1 |

（未完待续）


