---
layout: post
title: 文本分类
category: 计算机
tags: 数据库
keywords: 
description: 
---




# CNN在文本分类中的应用
## 字符级别的识别      
早期深度学习在自然语言上的应用比较暴力，直接把数据往CNN里怼。[文章Text Understanding from Scratch](https://arxiv.org/abs/1502.01710)解释了为啥子CNN也能对文本分类：它先对字符集做了一个类似盲文的编码，将字符编码为定长（l）的向量，然后送入CNN网络来分类。       
![Text Understanding from scratch](https://raw.githubusercontent.com/anxingle/anxingle.github.io/77839987b9aaf5ac1dcc0403874b325513260496/public/img/ML/text_1.png)          
          文章厉害的地方在于直接把所有的文本（中文换成拼音）直接怼进去，然后就能取得很厉害的分类结果。表示怀疑，有空了重复实验。还有一篇文章[CNN for Sentence Classification](https://arxiv.org/abs/1408.5882)稍有改进，把文本进行word embedding后，再送入了CNN。
![CNN for sentence Classification](https://raw.githubusercontent.com/anxingle/anxingle.github.io/77839987b9aaf5ac1dcc0403874b325513260496/public/img/ML/text_2.png),Max-pooling后得到固定长度的feature map。           
[A C-LSTM for Text Classification](https://arxiv.org/abs/1511.08630)更进一步，将卷机后的feature maps送入了window feature sequence后再送入LSTM。<img src="https://raw.githubusercontent.com/anxingle/anxingle.github.io/77839987b9aaf5ac1dcc0403874b325513260496/public/img/ML/text_3.png" width="500">    
优点在于既能捕获局部特征，又能学习到语义表达。不过针对其他的RNN，CNN变形结构，没有什么明显的优势。       
## 句子级别的识别。    
循环（Recurrent）卷机神经网络针对句子过长时，网络无法有效结合上下文信息来表达信息,创造性地提出结合word的上下文来表达每个word的信息 ![一图胜千言](https://raw.githubusercontent.com/anxingle/anxingle.github.io/77839987b9aaf5ac1dcc0403874b325513260496/public/img/ML/text_4.png)        
公式表述也很简洁![公式](https://raw.githubusercontent.com/anxingle/anxingle.github.io/master/public/img/ML/text_6.png)。      
## 文档级别
最近大热的注意力机制很是风骚啊，[Hierarchical attention networks for Document Classification
](https://arxiv.org/abs/1606.02393)这篇文章最屌的地方在于可以对复杂句进行分类，明明看上去像是褒义的句子，但是它能够辨识出这是反讽！精度上就更不用说了。       
<img src="https://raw.githubusercontent.com/anxingle/anxingle.github.io/77839987b9aaf5ac1dcc0403874b325513260496/public/img/ML/text_5.png" width="500">
。    
虽然图画的很复杂，但是代码还真心没有几行的。我们可以再来分析分析它的代码。
**太忙了，占坑，过几天再填**